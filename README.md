# Recurrent-Neural-Network

Implementations of a general Recurrent Neural Network classes that is trained using the Adam optimizer implemented by Tensorflow. We will use Dynamics Components Analysis and other statistical analysis techniques to map its activity onto low-dimensional subspaces and understand how network dynamics change during and after the training period.

Gradient descent method based on the paper "Training Excitatory-Inhibitory Recurrent Neural Networks for Cognitive Tasks: A Simple and Flexible Framework" (2016) by Song et. al.

We have done initial analysis on tasks similar to those used in the paper, "Task representations in neural networks trained to
perform many cognitive tasks" by Yang et. al. (2019). 

In the future, we are planning to expand on a technique called 'slow point analysis' to define a measure of task complexity. The technique is descrived in the paper, 

"Universality and individuality in neural dynamics across large populations of recurrent networks" (2019) by Maheswaranathan et al. 

as well as in the following course notes: https://web.stanford.edu/class/cs379c/archive/2016/calendar_invited_talks/articles/SussilloandBarakNC-13.pdf
