# Recurrent-Neural-Network
Implementations of a general Recurrent Neural Network classes that is trained using the Adam optimizer implemented by Tensorflow. We will use Dynamics Components Analysis and other statistical analysis techniques to map its activity onto low-dimensional subspaces and understand how network dynamics change during and after the training period.

Gradient descent method based on the paper "Training Excitatory-Inhibitory Recurrent Neural Networks for Cognitive Tasks: A Simple and Flexible Framework" (2016) by Song et. al.

See the initial results at init_results.pdf for more information.

We have done initial analysis on tasks similar to those used in the paper, "Task representations in neural networks trained to
perform many cognitive tasks" by Yang et. al. (2019). 

